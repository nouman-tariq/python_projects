{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "tracking.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nouman-tariq/python_projects/blob/main/tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLvNSJtAxHlL"
      },
      "source": [
        "## Computer Vision - lab 7 \n",
        "### Read, Write and Display a video using OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qdAJ9tMxHlO"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv \n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHGHLpp_xHlP"
      },
      "source": [
        "### Reading and displaying a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cld6YnbCxHlP",
        "outputId": "a69d1b4b-1d5c-4a42-9a5d-21076c459978"
      },
      "source": [
        "base_folder = \"data\"\n",
        "path_video1 = os.path.join(base_folder, \"3.mp4\")\n",
        "\n",
        "# Open the video \n",
        "cap = cv.VideoCapture(path_video1) \n",
        "\n",
        "if cap.isOpened() == False: \n",
        "    print(\"Error opening video stream or file\") \n",
        "    \n",
        "frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "print('frame_width = ' + str(frame_width))\n",
        "frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "print('frame_height = ' + str(frame_height))\n",
        "\n",
        "length = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "print(\"total number of frames = \" + str(length))\n",
        "\n",
        "fps = int(cap.get(cv.CAP_PROP_FPS))\n",
        "print(\"frames per second = \" + str(fps))\n",
        "\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frame_width = 1280\n",
            "frame_height = 720\n",
            "total number of frames = 196\n",
            "frames per second = 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQO54qmfxHlR"
      },
      "source": [
        "# Display the video\n",
        "def display_video(video_path: str, max_number_of_frame_to_run: int = None) -> None:\n",
        "    \"\"\"\n",
        "    This function display the video of the screen frame by frame.\n",
        "    :param video_path: Path to the video\n",
        "    :param max_number_of_frame_to_run: Set how many frame to be displayed. If None all frames will be displayed.\n",
        "    \"\"\"\n",
        "    \n",
        "    current_frame = 0 \n",
        "    \n",
        "    # Open the video \n",
        "    cap = cv.VideoCapture(path_video1)\n",
        "\n",
        "    while cap.isOpened(): \n",
        "\n",
        "        ret, frame = cap.read() # Read the frame \n",
        "        if ret is True:\n",
        "            current_frame = current_frame + 1 \n",
        "            cv.imshow(\"Frame\", frame)\n",
        "\n",
        "            if max_number_of_frame_to_run is not None and current_frame > max_number_of_frame_to_run:\n",
        "                break\n",
        "\n",
        "            if cv.waitKey(25) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # after playing the video, release the video capture    \n",
        "    cap.release()\n",
        "    # close all the frames\n",
        "    cv.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLT9ocAxxHlS"
      },
      "source": [
        "display_video(video_path=os.path.join(base_folder, \"3.mp4\"), max_number_of_frame_to_run=750)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqgSf2LSxHlS"
      },
      "source": [
        "### Writing a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqZIZFnPxHlS"
      },
      "source": [
        "def read_frames(video_path):\n",
        "    \"\"\"\n",
        "    This function takes the video path and returns the a list of frames.\n",
        "    :param video_path: Path to the video\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    cap = cv.VideoCapture(video_path)  \n",
        "    if cap.isOpened() == False: \n",
        "        raise Exception(\"Error opening video stream or file\") \n",
        "        return frames\n",
        "    \n",
        "    while cap.isOpened():  \n",
        "        ret, frame = cap.read() # Read the frame\n",
        "        if ret is True:\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmjLW9f9xHlT"
      },
      "source": [
        "# We are going to write the same video at 1 fps, first we need to read the frames.\n",
        "frames = read_frames(os.path.join(base_folder, \"3.mp4\"))\n",
        "\n",
        "# here we have the extensions and the fourcc for each of it\n",
        "video_extension_and_fourcc_dict = {'avi': cv.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
        "                                   'mp4': 0x7634706d}   \n",
        " \n",
        "# We need to create a VideoWriter object. \n",
        "# First, we should specify the output file name with its format (eg: 1_fps_1.mp4). \n",
        "# We should specify the FourCC code and the number of frames per second (FPS). \n",
        "# Lastly, the frame size should be passed (width, height).\n",
        "\n",
        "video_output_name = \"3_fps_1.mp4\"\n",
        "output_video = cv.VideoWriter(video_output_name, video_extension_and_fourcc_dict[\"mp4\"], 1,\n",
        "                              (frames[0].shape[1], frames[0].shape[0]))\n",
        "\n",
        "num_frames = len(frames)\n",
        "# We know that the first video has 30 fps.\n",
        "for i in range(0, num_frames, 30):\n",
        "    output_video.write(frames[i]) # writing the frame\n",
        "\n",
        "# don't forget to release the video writer\n",
        "output_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP0lfkx2xHlU"
      },
      "source": [
        "# Video analysis of a snooker footage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac7k8_4sxHlU"
      },
      "source": [
        "Goal was to develop an automatic system for video analysis of snooker footages. The system should be able to detect the snooker table and the balls on the table, track the balls, infer when a ball is potted into a pocket. We'll focus about tracking the cue ball (white ball) and another ball. The initial bounding boxes of the two balls are provided for the first frame (they follow the format [xmin ymin xmax ymax], where (xmin, ymin) is the top left corner and (xmax, ymax) is the bottom right corner of the initial bounding-box). In a video, we consider that the algorithm correctly tracks a ball if in more (greater or equal) than 80% of the frames the algorithm correctly localizes the ball to be tracked. We consider that the algorithm correctly localizes the ball to be tracked in a specific frame if the value of the IOU (intersection over union) between the window provided by your algorithm and the ground-truth window is more than 20%. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuZxS1tdxHlV"
      },
      "source": [
        "## Tracking using template matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMxCGsKixHlX"
      },
      "source": [
        "def select_roi(frame):\n",
        "    \"\"\"\n",
        "    Select the roi from the image.\n",
        "    :param frame\n",
        "    :return roi, x, y, w, h\n",
        "    \"\"\"\n",
        "    x, y, w, h = cv.selectROI(frame) \n",
        "    track_window = (x, y, w, h) \n",
        "    roi = frame[y: y + h, x: x + w]\n",
        "    \n",
        "    annotated_frame = cv.rectangle(frame, (x, y), (x + w, y + h), 255, 2) \n",
        "    cv.imshow('First frame initialization', annotated_frame)\n",
        "    cv.waitKey(2000)\n",
        "    cv.destroyAllWindows()\n",
        "         \n",
        "    return roi, x, y, w, h\n",
        "\n",
        "def find_ball_using_template_matching(frame, roi_gray, old_bbox):\n",
        "    \"\"\"\n",
        "    :param frame: Current frame\n",
        "    :param roi_gray: the previous detected region\n",
        "    :param old_bbox: the previous detected bbox corresponding to roi_gray [x, y, w, h].\n",
        "    :return new_x, new_y\n",
        "    \"\"\"\n",
        "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "    x, y, w, h = old_bbox\n",
        "    \n",
        "\n",
        "    # define the searching region for template matching\n",
        "    # a rectangular region 4h * 4w\n",
        "    center = (y + h//2, x + h//2)\n",
        "    y_min = np.max((0, center[0] - (2*h)))\n",
        "    y_max = np.min((frame.shape[0], center[0] + (2*h)))\n",
        "    x_min = np.max((0, center[1] - (2*w)))\n",
        "    x_max = np.min((frame.shape[1], center[1] + (2*w)))\n",
        "    \n",
        "    # display the searching region\n",
        "    mask1 = np.int8(np.zeros(frame_gray.shape))\n",
        "    mask1[y_min: y_max, x_min: x_max] = 255 \n",
        "    frame_gray_mask = cv.bitwise_and(frame_gray,frame_gray,mask=mask1)\n",
        "    cv.imshow('frame gray mask', frame_gray_mask)\n",
        "    cv.waitKey(500)\n",
        "\n",
        "    # function cv.matchTemplate works only on grayscale templates\n",
        "    res = cv.matchTemplate(frame_gray_mask, roi_gray, cv.TM_CCOEFF_NORMED)        \n",
        "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
        "\n",
        "    new_y = max_loc[1]\n",
        "    new_x = max_loc[0]\n",
        "    return new_x, new_y\n",
        "    \n",
        "def track_ball_using_template_matching(video_path) -> list:\n",
        "    \"\"\"\n",
        "    This function track the ball (which is initialized using select ROI) using template matching:\n",
        "    Template matching: https://docs.opencv.org/master/df/dfb/group__imgproc__object.html#ga586ebfb0a7fb604b35a23d85391329be\n",
        "    :param video_path: The path to the video.\n",
        "    :return bounding boxes where the ball was found.\n",
        "    \"\"\"\n",
        "    \n",
        "    bboxes = []\n",
        "    \n",
        "    cap = cv.VideoCapture(video_path)\n",
        "    ret, first_frame = cap.read() # Read the first frame \n",
        "    \n",
        "    roi, x, y, w, h = select_roi(first_frame)  \n",
        "    roi_gray = cv.cvtColor(roi, cv.COLOR_BGR2GRAY)\n",
        "    frame_idx = 0\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        frame_idx += 1\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret is True: \n",
        "            x, y = find_ball_using_template_matching(frame, roi_gray, old_bbox=[x, y, w, h])\n",
        "            bboxes.append([frame_idx, x, y, x + w, y + h])\n",
        "            \n",
        "            # show annotated image with new bbox\n",
        "            annotated_image = cv.rectangle(frame, (x, y), (x + w, y + h), 255, 2)\n",
        "            cv.imshow('annotated_image', annotated_image)\n",
        "\n",
        "            if cv.waitKey(25) & 0xFF == ord('q'):\n",
        "                break\n",
        "                \n",
        "        else:\n",
        "            break  \n",
        "            \n",
        "    cap.release() \n",
        "    cv.destroyAllWindows()\n",
        "    return bboxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RKFA9OtxHlZ"
      },
      "source": [
        "base_folder = \"data\"\n",
        "# 3, 4, 8, 9, 11, 19, 20\n",
        "path_video1 = os.path.join(base_folder, \"3.mp4\")\n",
        "bboxes = track_ball_using_template_matching(path_video1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc4vUQLfxHla"
      },
      "source": [
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the intersection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "def compute_percentage_tracking(gt_bboxes, predicted_bboxes, num_frames):\n",
        "    \"\"\"\n",
        "    This function compute the percentage of detected bounding boxes based on the ground-truth bboxes and the predicted ones.\n",
        "    :param gt_bboxes. The ground-truth bboxes with the format: frame_idx, x_min, y_min, x_max, y_max.\n",
        "    :param predicted_bboxes. The predicted bboxes with the format: frame_idx, x_min, y_min, x_max, y_max\n",
        "    :param num_frames. The total number of frames in the video.\n",
        "    \"\"\"\n",
        "    \n",
        "    num_frames = int(num_frames)\n",
        "    \n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    \n",
        "    gt_dict = {}\n",
        "    for gt_box in gt_bboxes:\n",
        "        gt_dict[gt_box[0]] = gt_box[1:]\n",
        "    \n",
        "    pred_dict = {}\n",
        "    for pred_bbox in predicted_bboxes:\n",
        "        pred_dict[pred_bbox[0]] = pred_bbox[1:]\n",
        "        \n",
        "    for i in range(num_frames):\n",
        "        if gt_dict.get(i, None) is None and pred_dict.get(i, None) is None: # the ball is not on the table\n",
        "            tp += 1 \n",
        "        \n",
        "        elif gt_dict.get(i, None) is not None and pred_dict.get(i, None) is None: # the ball is not detected\n",
        "            fp += 1\n",
        "            \n",
        "        elif gt_dict.get(i, None) is None and pred_dict.get(i, None) is not None: # the ball is not on the table, but it is 'detected'\n",
        "            fp += 1\n",
        "            \n",
        "        elif gt_dict.get(i, None) is not None and pred_dict.get(i, None) is not None: # the ball is on the table and it is detected\n",
        "            \n",
        "            iou = bb_intersection_over_union(gt_dict[i], pred_dict[i])\n",
        "            if iou >= 0.2:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1 \n",
        "             \n",
        "            \n",
        "    print(f'tp = {tp}, fp = {fp}')\n",
        "    assert tp + fp == num_frames\n",
        "    perc = tp / (tp + fp)\n",
        "    \n",
        "    return perc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAdvdKpxHlc",
        "outputId": "8790c09e-242e-4bbf-a358-af0e81f33f4e"
      },
      "source": [
        "# load the ground-truth file\n",
        "black_ball_gt = np.loadtxt(os.path.join(base_folder, '3_ball_2_gt.txt'))\n",
        "# the first line contains the lenght (number of frames) of the video (followed by -1 in order to keep the dimension of the array)\n",
        "print(black_ball_gt[0])\n",
        "\n",
        "white_ball_gt = np.loadtxt(os.path.join(base_folder, '3_ball_1_gt.txt'))\n",
        "print(white_ball_gt[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[196.  -1.  -1.  -1.  -1.]\n",
            "[196.  -1.  -1.  -1.  -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq4HHWYpxHld",
        "outputId": "ebf766b3-85a3-4961-8321-a6fb586124f4"
      },
      "source": [
        "compute_percentage_tracking(white_ball_gt[1:], bboxes, white_ball_gt[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tp = 190, fp = 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9693877551020408"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOS-rK6QxHld"
      },
      "source": [
        "## Color histrogram for detecting the snooker balls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpiV_MtSxHle"
      },
      "source": [
        "def compute_color_histogram(img, bins_0, bins_1, bins_2):\n",
        "    \"\"\"\n",
        "    Compute the color histogram of a window in the quantized BGR color space.\n",
        "    :param img. \n",
        "    :param bins_0. The number of bins for the first channel.\n",
        "    :param bins_1. The number of bins for the second channel.\n",
        "    :param bins_2. The number of bins for the third channel.\n",
        "    :return The histogram.\n",
        "    \"\"\"\n",
        "    \n",
        "    histogram = np.zeros((bins_0, bins_1, bins_2))\n",
        "    # TODO: create the histogram by placing each b, g, r value to the corresponding bin.\n",
        "        \n",
        "    return histogram.flatten()\n",
        " \n",
        "def compute_color_histogram_spatial_pyramid(img, bins_0, bins_1, bins_2):\n",
        "    \"\"\"\n",
        "    Spatial pyramid with one level: the initial color histograsms + the color histograms of the 4 subimages.\n",
        "    :param img. \n",
        "    :param bins_0. The number of bins for the first channel.\n",
        "    :param bins_1. The number of bins for the second channel.\n",
        "    :param bins_2. The number of bins for the third channel.\n",
        "    :return The histogram.\n",
        "    \"\"\"\n",
        "    \n",
        "    histogram1 = compute_color_histogram(img, bins_0, bins_1, bins_2)\n",
        "    \n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    \n",
        "    histogram2 = compute_color_histogram(img[:h//2, :w//2], bins_0, bins_1, bins_2)\n",
        "    histogram3 = compute_color_histogram(img[h//2: h, w//2: w], bins_0, bins_1, bins_2)\n",
        "    histogram4 = compute_color_histogram(img[h//2: h, :w//2], bins_0, bins_1, bins_2)\n",
        "    histogram5 = compute_color_histogram(img[h//2: h, w//2:w], bins_0, bins_1, bins_2)        \n",
        "    \n",
        "    histogram = np.array([histogram1, histogram2, histogram3, histogram4, histogram5])\n",
        "    \n",
        "    return histogram.flatten()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haQUM6HpxHle"
      },
      "source": [
        "# Select ROI\n",
        "video_path = os.path.join(base_folder, \"3.mp4\")\n",
        "cap = cv.VideoCapture(video_path)\n",
        "ret, first_frame = cap.read() # Read the first frame \n",
        "cap.release()\n",
        "\n",
        "img_crop, _, _, _, _ = select_roi(first_frame)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhO5IYhrxHle",
        "outputId": "3b133f99-5f2f-48dc-b326-df070d745a42"
      },
      "source": [
        "# compute the color histogram of the window in the quantized BGR color space\n",
        "# use our function\n",
        "histogram_img = compute_color_histogram(img_crop, 4, 4, 4)\n",
        "print('Color histogram in the quantized BGR: 4, 4, 4')\n",
        "print(histogram_img.shape)\n",
        "print(histogram_img.sum())\n",
        "print(img_crop.shape[0] * img_crop.shape[1])\n",
        "print(histogram_img)\n",
        "\n",
        "print('Color histogram in with special pyramid: 4, 4, 4')\n",
        "histogram_img = compute_color_histogram_spatial_pyramid(img_crop, 4, 4, 4)\n",
        "print(histogram_img.shape)\n",
        "print(histogram_img.sum())\n",
        "\n",
        "\n",
        "# compute the color histogram of the window in the quantized BGR color space\n",
        "# use the function provided by OpenCV\n",
        "print('Color histogram (from OpenCV) in the quantized BGR: 4, 4, 4')\n",
        "hist_img = cv.calcHist([img_crop], [0, 1, 2], None, [4, 4, 4], [0, 256, 0, 256, 0, 256]) \n",
        "print(hist_img.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Color histogram in the quantized BGR: 4, 4, 4\n",
            "(64,)\n",
            "1020.0\n",
            "1020\n",
            "[ 44.   0.   0.   0.  70.  38.   0.   0. 333.  58.   4.   0.   1.   2.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  30.   0.\n",
            "   0.  11.  38.   8.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  19. 177. 183.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "Color histogram in with special pyramid: 4, 4, 4\n",
            "(320,)\n",
            "2040.0\n",
            "Color histogram (from OpenCV) in the quantized BGR: 4, 4, 4\n",
            "[ 44.   0.   0.   0.  70.  38.   0.   0. 333.  58.   4.   0.   1.   2.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  30.   0.\n",
            "   0.  11.  38.   8.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  19. 177. 183.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcFbFueBxHlf"
      },
      "source": [
        "## Tracking using particle filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYu5406bxHlf"
      },
      "source": [
        "def create_uniform_particles(x_range, y_range, N):\n",
        "    \"\"\"\n",
        "    A particle is a bounding box, represented by the top left corner and fixed width and height\n",
        "    \"\"\"\n",
        "    particles = np.empty((N, 2))\n",
        "    particles[:, 0] = uniform(x_range[0], x_range[1], size=N)\n",
        "    particles[:, 1] = uniform(y_range[0], y_range[1], size=N) \n",
        "    return particles\n",
        "\n",
        "\n",
        "def predict(particles, velocity, std, frame, w, h):\n",
        "    \"\"\"\n",
        "    Predict where the particles will be at the next frame by applying some dynamics\n",
        "    take into account velocity and some random noise.\n",
        "    \"\"\"\n",
        "    \n",
        "    N = len(particles)    \n",
        "    \n",
        "    noise = np.random.randn(N) * std[0]  \n",
        "    for i in range(N):\n",
        "        particles[i, 0] = particles[i, 0] + velocity[0] + noise[i]\n",
        "        # check that the particle is not outside of the image\n",
        "        if(particles[i, 0] > frame.shape[1] - w):\n",
        "            particles[i, 0] = frame.shape[1] - w\n",
        "        if(particles[i, 0] < 0):\n",
        "            particles[i, 0] = 0\n",
        "            \n",
        "    noise = np.random.randn(N) * std[1]\n",
        "    for i in range(N):\n",
        "        particles[i, 1] = particles[i, 1] + velocity[1] + noise[i]\n",
        "        if(particles[i, 1] > frame.shape[0] - h):\n",
        "            particles[i, 1] = frame.shape[0] - h\n",
        "        if(particles[i, 1] < 0):\n",
        "            particles[i, 1] = 0 \n",
        "    return particles\n",
        "\n",
        "\n",
        "def update(particles, frame, hist_roi_norm, w, h):\n",
        "    \"\"\"\n",
        "    Update the weight of each particle based on how similar is to the target window \n",
        "    use a simple color histogram model essential step: how to update the weights.\n",
        "    \"\"\"\n",
        "    particles = np.int32(particles)   \n",
        "    weights = np.zeros((particles.shape[0]))\n",
        "    for i in range(particles.shape[0]):\n",
        "        img_particle = frame[particles[i, 1]: particles[i, 1] + h - 1, particles[i, 0]:particles[i, 0] + w - 1].copy()\n",
        "        particle_hist = cv.calcHist([img_particle], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])  \n",
        "        # do normalization\n",
        "        particle_hist_norm = particle_hist / particle_hist.sum()        \n",
        "      \n",
        "        weights[i] = cv.compareHist(hist_roi_norm, particle_hist_norm, cv.HISTCMP_CHISQR_ALT)\n",
        " \n",
        "        weights[i] = np.exp(-2 * (weights[i] ** 2))  \n",
        "    weights += 1.e-10 # avoid round-off to zero\n",
        "    # normalize the wights such that we have a probability distribution\n",
        "    weights /= sum(weights) \n",
        "    return weights\n",
        "\n",
        "\n",
        "def estimate(particles, weights):   \n",
        "    \"\"\"\n",
        "    Estimate the center of the cloud of particles.\n",
        "    \"\"\"\n",
        "    mean = np.float64(np.array([0, 0]))\n",
        "    N = particles.shape[0]  \n",
        "    for i in range(N): \n",
        "        mean += weights[i] * particles[i, :]   \n",
        "    return mean\n",
        "\n",
        "\n",
        "def resample(weights):\n",
        "    \"\"\"\n",
        "    Resample particles based on their weight.\n",
        "    \"\"\"\n",
        "    w = weights.flatten()\n",
        "    N = len(w)    \n",
        "    tries = np.random.multinomial(N, w)  \n",
        "    indexes = np.zeros(N, 'i')\n",
        "    cumsum_vector = np.cumsum(tries)\n",
        "    pos = -1 \n",
        "    for i in range(len(tries)):\n",
        "        for j in range(tries[i]):            \n",
        "            pos = pos + 1\n",
        "            indexes[pos] = i\n",
        "            \n",
        "    return indexes\n",
        "\n",
        "def resample_from_index(particles, weights, indexes):\n",
        "    \"\"\"\n",
        "    Using the new indexes, keep only the particles and the weights corresponsing to the indexes. Then, re-normalize the weights.\n",
        "    \"\"\"\n",
        "    particles[:] = particles[indexes]\n",
        "    weights[:] = weights[indexes]\n",
        "    weights /= np.sum(weights)\n",
        "    return particles, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpiRxkhHxHlg"
      },
      "source": [
        "def track_ball_using_particle_filtering(video_path: str, max_number_of_frame_to_run: int=None, num_particles=100):\n",
        "    \"\"\"\n",
        "    Track the ball using particle filtering using the following steps:\n",
        "    1. Generate the first particle using the first initialization of the ball.\n",
        "    2. Predict the new locations of the particles using the velocity and some noise.\n",
        "    3. Compute the weight of each particle based on their similarity with the first roi.\n",
        "    4. Get the new location of the ball using a weighed sum of the particles.\n",
        "    5. Resample particles based on their weight (the weight of a particle means the similarity with the first roi).\n",
        "    \n",
        "    :param video_path. Path to the video.\n",
        "    :param max_number_of_frame_to_run. How many frames to track the ball.\n",
        "    :param num_particles. How many particle to have.\n",
        "    \"\"\"\n",
        "    cap = cv.VideoCapture(video_path)  \n",
        "    current_frame = 0\n",
        "    max_number_of_frame_to_run = 750\n",
        "\n",
        "    ret, first_frame = cap.read() # Read the first frame         \n",
        "    img_roi, x, y, w, h = select_roi(first_frame) \n",
        "\n",
        "    hist_roi = cv.calcHist([img_roi], [0 ,1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])  \n",
        "    hist_roi_norm = hist_roi / hist_roi.sum() \n",
        "    bb = np.array([x, y, x + w, y + h])\n",
        "\n",
        "    particles = ...\n",
        "    velocity = [0, 0]\n",
        "    std = [25, 25] \n",
        "    bboxes = []\n",
        "\n",
        "    while(cap.isOpened()): \n",
        "        ret, frame = cap.read() # Read the frame   \n",
        "        if ret is True: \n",
        "            current_frame = current_frame + 1   \n",
        "            particles = ...     \n",
        "\n",
        "            weights = ...\n",
        "\n",
        "            obj = np.int32(...)       \n",
        "            bboxes.append([current_frame, obj[0], obj[1], obj[0] + w, obj[1] + h])   \n",
        "\n",
        "            velocity[0] = obj[0] - bb[0]\n",
        "            velocity[1] = obj[1] - bb[1]      \n",
        "            bb = obj.copy()\n",
        "\n",
        "            indexes = ...\n",
        "            particles, weights = ...          \n",
        "\n",
        "            annotated_image = frame.copy()\n",
        "            for i in range(num_particles):            \n",
        "                annotated_image = cv.rectangle(annotated_image, (np.int32(particles[i, 0]), np.int32(particles[i, 1])), (np.int32(particles[i, 0]) + w, np.int32(particles[i, 1]) + h), (0, 255, 0), 1)\n",
        "\n",
        "            annotated_image = cv.rectangle(annotated_image, (obj[0], obj[1]), (obj[0] + w, obj[1] + h), (0, 255, 255), 4)\n",
        "            cv.imshow('annotated_image', annotated_image)   \n",
        "            cv.waitKey(100)\n",
        "            \n",
        "            if max_number_of_frame_to_run is not None and current_frame > max_number_of_frame_to_run:\n",
        "                break\n",
        "\n",
        "            if cv.waitKey(25) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # after playing the video, release the video capture    \n",
        "    cap.release()                \n",
        "    # close all the frames\n",
        "    cv.destroyAllWindows()\n",
        "    return bboxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZU5SJPtxHlg"
      },
      "source": [
        "# change the path to match on your machine\n",
        "base_folder = 'data'\n",
        "video_path = os.path.join(base_folder, \"20.mp4\")\n",
        "bboxes = track_ball_using_particle_filtering(video_path, max_number_of_frame_to_run=750, num_particles=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLphKCUTxHlh",
        "outputId": "e1dbb581-cac7-4e75-f1be-71bcefcba932"
      },
      "source": [
        "# load the ground-truth file\n",
        "red_ball_gt = np.loadtxt(os.path.join(base_folder, '20_ball_2_gt.txt'))  \n",
        "white_ball_gt = np.loadtxt(os.path.join(base_folder, '20_ball_1_gt.txt')) \n",
        "compute_percentage_tracking(white_ball_gt[1:], bboxes, white_ball_gt[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tp = 235, fp = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9957627118644068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCa5olnqxHlh"
      },
      "source": [
        "cv.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhbH2q48xHlh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}